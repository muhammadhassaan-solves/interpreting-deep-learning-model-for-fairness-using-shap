# -*- coding: utf-8 -*-
"""SHAPFairness.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RO3LA4yg_5dUzXlTAHizxHXxigRzz_pK
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import tensorflow as tf

# Load dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
column_names = ["age", "workclass", "fnlwgt", "education", "education-num", "marital-status",
                "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss",
                "hours-per-week", "native-country", "income"]
data = pd.read_csv(url, names=column_names)

# Data preprocessing
data = data.apply(LabelEncoder().fit_transform)
X = data.drop('income', axis=1)
y = data['income']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_acc}')

!pip install shap

import shap

# Initialize the SHAP explainer
explainer = shap.KernelExplainer(model.predict, X_train[:100])  # Using a subset for efficiency

# Compute SHAP values for the test set
shap_values = explainer.shap_values(X_test[:100])  # Using a subset for efficiency

# Visualize the SHAP values for a single prediction
shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0], X_test[:100])

# Summarize feature importance
shap.summary_plot(shap_values, X_test[:100])



# SHAP summary plot to visualize feature importance
shap.summary_plot(shap_values, X_test[:100], feature_names=data.columns[:-1])

import numpy as np

feature_importance = np.abs(shap_values).mean(axis=0)

for name, val in zip(X.columns, feature_importance):
    print(f"{name}: {np.mean(val):.4f}")

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Reweight the dataset to address bias
weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = dict(enumerate(weights))

# Re-train the model with class weights
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weights)

import shap

# New SHAP explainer with the updated model
explainer = shap.KernelExplainer(model.predict, X_train[:100])
shap_values = explainer.shap_values(X_test[:100])

import numpy as np

# Ensure SHAP values are in the correct shape
feature_importance = np.abs(shap_values).mean(axis=0)

print("Feature Importance (After Reweighting):\n")
for name, val in zip(X.columns, feature_importance):
    # If val is an array (like [0.123]), convert to float
    if isinstance(val, np.ndarray):
        val = val.item()
    print(f"{name}: {val:.4f}")